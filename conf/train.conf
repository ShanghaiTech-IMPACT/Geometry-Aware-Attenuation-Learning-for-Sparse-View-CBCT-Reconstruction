# config file for train
model {
    fusion = ada  # max, mean, var, ada, local
    aggregator{
        activation = GELU
        latent_size = 256
    }
    encoder {
        num_layers = 4
        use_first_pool = False
        dim_in = 1
        inplanes = 16
        feat_num_list = [16,32,64,128]
        latent_size = 256
        input_size = [256,256]
        layer_num_list = [3, 4, 6, 3]
        activation = GELU  # GELU/ReLU
        block = BasicBlock
        normalization = Batch # Instance/Batch
    }
    SRGAN{
        generator{
            scale = 4
            inplanes = 256
            res_blk_num = 6
            channel_reduce_factor = 4
            activation = GELU  # GELU/ReLU
            normalization = Batch # Instance/Batch
        }
    }
    last_layer
    {
        act = GELU # ReLU/GELU, we recommend GELU, otherwise it will dead.
    }     
    
}
render {
    ray_batch_size = 1024  # render ray batch size during training
    factor = 0.5 # uniform sample interval 
    chunksize = 65536 # render ray chunk size during evaluating
}
train {
    G_loss{
        mse_lambda_2d = 0.01
        mse_lambda_3d = 1
        gd1_lambda = 1
    }
    print{
        save_interval = 10
        val_interval = 10
        test_interval = 10
        vis_interval = 10
    }
}
data {
    # The min, max value for attenuation
    dental{
        clamp_min = 0.0
        clamp_max = 0.09009
    }
    spine{
        clamp_min = 0.0
        clamp_max = 0.051744
    }
    Walnuts{
        clamp_min = 0.0
        clamp_max = 0.084
    }
}
lr_sche {
    init_lr=0.0001
    step_size=50
    gamma=0.5
}